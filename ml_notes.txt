BASIC PROBABILITY
=================

mean = sum(points) / count(points)
variance = sum(square(point-mean)) / count(points)
covariance = sum(1/n (x-xmean) * (y-ymean)), talks about how x and y change together
std = sqrt(variance)
std error - if we take multiple data samples from population, we can calc std of means

intuition behind using prior & posterior distr :
	if we assume a random variable x follows a guassian prior with 0 mean and covariance that can be explained by a function f(x) 
	then we can use guassian process to estimate its posterior (prediction). 
	Since x is a random variable, it can take multiple paths from time t into future. 
	This is similar to multivariate distribution. 
	Given a set of finite data points X for random variable x, we can predict future values of x by using for kernel function f(x).

Pdf - probability density for a continuous random variable P(x-e < X < x+e)
Cdf - integral of Pdf - probability upto a particular value of the random variable P(X<=x)
hypothesis tests - statistical significance, p-value
p-value - hypothesis test that verifies if observation is purely by chance (<0.05 means null hypothesis is invalid)
if z score (static table of values) in critical regions then null hypothesis is rejected
confidence intervals +/- around p value
uniform distribution is in range [-m, m)
normal distribution has mean +/- variance
central limit theorem (CLT) - frequency plots of means will be normal distribution, lesser std if more means

UNDERSTAND DATA
===============
Sampling from raw data = 
	with replacement means you replace the selected data point back into dataset for future selection 
	without replacement means once a data point is selected we cannot select it again
Repeated K-fold Cross Validation (practical)
	take full dataset, split into k sections at random
	train on k-1, test on kth fold - repeat several times & avg
	leads to relatively low variance and small bias
Bootstrapping (practical)
	dont want to use raw data, then rely on CLT & use bootstrapping for variance tests 
	select a part of raw data at random -- test set / evaluation set
	re-sample again and again WITH replacement from that part
	save means of each random sample & use it to verify variance
	causes high bias but low variance
	in case of biased data, re-scale to center of all stored sample means 
Over / Under sampling
	balance classes by synthetically adding more points for minority class
	balance classes by randomly selecting on few points from majority class
Stratified sampling
	when you have clusters of data - say views, atb, purchases
	random subsample from each cluster to create random sample
Inverse sampling
	to generate samples form a known pdf, say f(x)
	calculate inverse cdf of the known pdf
	feed from uniform(0,1) to the inverse cdf to generate random values of x
Rejection sampling
	to generate samples of non-normal pdf (say exponential)
	for 2d space, generate 2 values x & y from uniform space
	reject points above pdf, accept points below pdf
Importance sampling
	to generate samples of unknown pdf
	rewrite unknown pdf using known pdf (say normal distr) + some factor
	do WEIGHTED sampling from known pdf
Monte carlo simulation (theory)
	Unbiased estimators on some sampling technique (inverse, rejection etc)
	Metropolis random walk - 
		pick random point Xi from raw 
		pick random point from region around Xi -- Xj
		calc prob (Xj / X) <= uniform(0,1) to select Xj as next state
	Markov chain : xk = x0 * pow(P, k) where P is transition probabilities
	Markov chains stabilize after certain k at which point P needs to be recomputed
	MCMC is popular inference technique, other are Maximum likelihood estimate and Frequentist
Maximum likelihood estimate
	Find the parameter values that maximize the likelihood of observing the data given the model
	MLE uses optimization such as gradient descent, adam etc
	MLE is consistent, as data increases it converges to true value of data
	
Transform data - 
	apply log, exp, square root etc
	normalize = (x-min) / (max-min)
	normalize to [0, 1)
	standardize = (x-mean) / std
	remove skewness
	normalize for RF ; standradize for SVM ; Min-Max scale for KNN etc
		
	
T-test, ANOVA and Tukey (assume normal distribution)
T-test is to measure if 2 sample means are different - yes/no
ANOVA is to measure if 3 or more sample means are different - yes/no. 
It compares overall mean with individual means
Individual samples cannot have big diff in their variance + no outliers
If p-value is less than alpha (typically 0.05), then means are different because null hypothesis that both are same is invalid. 
But by how much? Use Tukey test which does all pairs comparison.
Always do tests on means of samples, not values in samples 
Always get data/samples from both user groups & time windows (same user group - trends, seasonality etc)


Chi square test = 
Mostly for categorical inputs
Sigma (y-y')2/y' --> to measure if variance in output is due to input or not


If two independent distributions X and Y & we want to check if point z is from X or Y
assume X , Y are all from indpendent normal distributions - p(X), p(Y) is given
p(zEX|X) = p(X|zEX).p(z) / P(X)
p(zEY|Y) = p(Y|zEX).p(z) / P(Y)
P(z) = p(zEX|X).p(X) + p(zEY|Y).p(Y) 
P(X and Y) = P(X) + P(Y) + P(X or Y)
	

DATA MINING
===========

Apriori - scan db multiple times to find patterns based on counts
	generates candidates & updates them
	good if unique items is small 
FP growth - scan db twice to build tree of paths + counts


Traditional ML
==============

Kmeans (unsupervised) - assign k labels to data at random ;
compute centroid for each cluster ; re-assign data to centroids based on distance ;
compute variance of cluster ; do repeatedly ; plot elbow curve


KNN (supervised) - have data with labels ; get new unlabeled data ; measure distances ;
pick top k labeled data ; give new label based on max of top k
	
	
Linear regression - used for gueustimating the relationship between two variables x and y as accurately as possible. 
If we normalize data, coeff will change. But result wont. Hence we can use data as-is. 
Naturally, there is some error while doing so. 
Typically, RMSE is used to prevent +ve, -ve errors canceling each other out. 

y = mx+c = weight*x + bias

Logistic regression - for classification into multiple output variables
relation between input and output variables follows a logistic funciton [0,1]. 
It assumes that log odds of y is linear function of x. 
Hence normalization of data helps. 
log(y/1-y) = mx+c
y = ez / 1 + ez

Linear, Logistic --> 
assume linear decision boundaries ; 
error variance is constant & errors are independent ; 
variables are not collinear	
If we violate assumptions then coeff or std err will vary & hence inconclusive
Use residual plots to test


Naive Bayes - probabilistic classification model that assumes features are independent.
P(x|y) = P(y|x).P(x) / P(y)


Random forest vs xgboost (classification and/or regression)
Decision trees rely on entropy which is average of variance 
	select nodes based on yes/no [or] avg values etc
	calculate info gain (or gini impurity) & pick feature / prune leaves
	info gain to measure the quality of a split, prev entropy - curr entropy
	entropy = -sigma(plogp) where p is prob of belonging to class
	repeat same on other features
	as tree depth increases we are making trees too specific
	as num trees increases we are making model learn (info gain) less and less
Rf is bagging technique, learn individual learners & avg. 
	Bootstapping is randomly selecting data from raw
	Build bunch of decision trees for randomly picked features from randomly picked data (bootstrap)
	Increase / Decrease number of picked features & do again
	Aggregate the results from all trees to finalize
	Bagging = bootstrap + aggregate to prevent overfit
	Check out-of-bag data accuracy (look at all trees) 
	Missing data for features need to be guessed
GBT is boosting technique, learn weak learners in sequence + use weights.
	Start with a random leaf & build RESIDUAL/ERROR tree using averages
	Calculate scores, gain & compare gain with some defined gamma to prune (or not)
	Continue building CHAIN of residual trees (more features)
	Each tree is small (say 4) 
	Aggregate values by averaging features (regression) / log odds (classification)
	Calculate predictions using a fixed learning rate (avoid overfit)
	XGBoost - advanced regularization while calculating scores & outputs
	
Trees --> assume linear decision boundaries ; 
can use data as is because impurity is calc like if-else & small weak learners are used. Normalizing data wont matter.


SVM (support vector machines) for classification -
support vectors are actual margins + small buffer to allow misclassication
move computations to higher dimensions using kernel functions & dot product
do cross validation with diff support vectors
rely on bootstrap for finding support vectors
supports linear & non-linear boundaries
SVM --> can do non-linear boundaries


REC SYS
=======
dimensionality reduction
	also known as feature projections in ML world - helps not needing to retrain always
	Eigen values, Eigen vectors - 
		when linear transforming data to lower dim, 
		eigen value is best multiplier such that
		M * eigenvector = eigenvalue * eigenvector
	tsne, pca -  
		PC is orthogonal to direction with most variance.
		pca is built on top of eigen vectors - deterministic
		tsne is probabilistic & hence non-deterministic
RFM analysis
	target users when recommending products
	look into user's recency, frequency, money
	score users
	cluster them to groups such as :
	  Can't Loose Them, Champions, Loyal, Needs Attention, Potential, Promising, Require Activation
colab filtering - item item, user user or user item.
matrix factorization - dim reduction. Like PCA. 
	Raw (mxn) = User (mxk) * Item (kxn) 
	Raw data = non-negative sparse matrix + coordinate descent
	User & Item = random init
	implicit data & weights
	RMSE loss function + regularization
implicit ALS -
	alternate between user and item vectors
	give weights to event types like view, atb, purchase
	L2 norm for regularization
	confidence calcualtions using imu/umi/filters/popularity
NCF - CF with DL
	have user factors and item factors as initial embed layers
	one path - matmul like traditional CF
	one path - multiple FC layers with RELU
	combine both paths & sigmoid 
	use dropouts where needed
Factorization Machines - 
	matrix factorization (counts) + features
	minimize distance, maximize uncertainity
Auto Encoders can be used to convert from N-dim space to M-dim space & back to N-dim space
	the hidden layer on M-dim space == PCA components
	but might not be orthogonal like PCA
	but can use non-linear activations (unlike PCA which is linear)
	AE arch = N input - k nonlinear active - M hidden - K nonlinear active - N output
Variational AE - context vector / M-dim space is from sampling
	use FC layers to transform input data in N-dim to N-dim
	sampling to M-dim (encode)
	decode from M-dim to N-dim


LOSS FUNCTIONS
==============
To calc max likelihood = minimize negative log likelihood
Listwise, Pairwise & Pointwise loss calculation --> maybe even use Triplet loss
Typically, loss = KL divergence, cross-entropy (for classification) [or] RMSE (regression)
RMSE is the standard deviation of the residuals (yi - ypredi)
Entropy = Negative log probability. 
Cross entropy is avg entropy size. 
Divergence is (cross entropy - entropy).
derivative of linear (y=mx+c) gives convex output -> RMSE works  
but derivative of logistic/sigmoid isnt convex -> RMSE does not work 
	hence we compute negative log(sigmoid)
contrastive loss -
	diff = positive sample - negative sample
	logsigmoid(diff) + regularize

NEURAL NETWORKS
===============

gradient descent - used to minimize the error in predicted output. 
Depends on math that derivative [or] rate of change of zero means that it is a point of local minima or maxima. 
batch mode = all at once ; stochastic = one point at a time ; mini batch = few points at a time (keep shuffling)
gradient descent = which direction towards 0 & how much step to take 
parallel gradient descent for large scale = https://joerihermans.com/ramblings/distributed-deep-learning-part-1-an-introduction/
We need to avoid local minima
Uses a param called learning rate which tells how quickly the algo should learn to converge. 
learning rate = size of the step to take (can be adaptive)
learning rate decay scheduled based on epochs
momentum = direction in which GD should keep going, based on previous directions
momentum adds fraction of previous decay output to current decay output calc, along with gradient
optimizer (for a cost function) = adam, sgd, gd, batch gd, momentum, adaptive learning rate ; 
	Adam is using decay in two ways - abs diff in gardient & squared diff in gradient. 
	Stochastic GD is compute cost at every step instead of cost for all.
cost function = sum (input * weight) + bias
initializing weights with normal distr & bias with 0 is common
non-linear activations to excite certain output 
	sigmoid activation = calc sum of negative log loss with truth
	tanh activation = steeper gradient than sigmoid
	relu activation = max(0, pred)  
	sigmoid [0,1] vs tanh[-1,1] vs relu[0,1]
	sigmoid, tanh have a vanishing gradient problem
	softmax activation = calc probability over all candidates 
			for output layers with multi-class
			formula = ei / sigma(e)
loss = RMSE (regression) or Cross Entropy (classification)
back prop = weighted sum of previous activations + bias (chain rule)
batch normalize after every few epochs or for every mini batch
learned params = weights & bias (both can have noise added to them to avoid overfit)
regularization (L1 and L2) of weights, dropout, more train data (maybe add noise)

CNN -
	Input, pooling, dropout, dense, normalize, activation
	typically used for classification
RNN, LSTM, GRU - 
	non-linear models (tanh / relu activations) 
	continuous representation of everything so far (previous output + current input)
	RNN has vanishing gradient problem when sequence is too long
	LSTM, GRU use sigmoid & tanh gates to remember/forget stuff 
	To predict a value, look at seq before the value & after the value -- BiLSTM
	cannot parallelize
Encode-Decoder -
	encoder goes thru time steps & puts everything to context vector
	decoder starts with a context vector and generates next time steps
	adding attention lets you focus on some of the many time steps
Attention - 
	compare with neighborhood to find which are important
	do matmul between embeddings and decider (say position)
	softmax to get areas of focus (probability on multi class)
	summarize (add) & give it to FFNN
	
DL --> 
can do non-linear boundaries ; 
needs data to be transformed into meaningful space because of error calc & weights adjustment ;


TIME SERIES
===========

ARIMA is simple (linear) time series forecasting technique
	based on moving averages & regression
	small data & immediate forecasting
	trends fit some distribution - add [or] multiply raw data with something
	seasonality repeats itself - need time window
	can try more complex EXPONENTIAL SMOOTHENING
	
HMM (Hidden Markov Models) for variable length time-series data -
	linear model to predict sequences using prior probabilities & state machines
	good when next state depends only on previous state (so define prev state carefully)
	use when final states are finite set
	has forward/backward learning but very simple model compared to DL
	
LSTM is the new kid on the block



NLP 
===
stopwords, stemming, lemmatization
POS tags can be created by building state machines on grammar rules
N-grams based bag-of-words model is typically step1 for any model like tfidf / lda
Tfidf computes (term freq) * log (inverse doc freq) -- can have various flavours
LDA -
	assumes dirchilect prior on topics; 
	samples words in doc using some distribution;
	calculates probabilities & repeats several times
transfer learning - 
	ability to learn one thing, say word embeddings
	use that knowledge in another final task, say text summarization
	final task needs fine tuning 
word2vec - 
	cbow vs skipgram : both used for context based prediction. 
	cbow prefers high frequency word patterns & it is faster.
	out of vocab cannot be handled - use char level embeddings / some bi-lstm
glove -
	similar to word2vec, generates word embeddings
big base models trained on massive data
	Bert - bidirectional, uses transformer model for sequence to sequence 
	GPT3 - unidirectional, uses transformer model with 100s of billions of params
zero/one/few shot learners -
	small num of train samples or no train samples for some label 
	but still okay because base model is GPT3 or something big


EVALUATION
==========

Make sure to feature engineer on TRAIN data, not ALL data
Standardize data in case of class imbalance
Cross validation - k fold, split data in k groups. Train on k-1 groups & test on the kth.
Bootstrap from test data chunk while measuring accuracy of model

Distance - eucledian, dot product, cosine similarity etc
dot product is angle & magnitude. cosine is angle only (norm of magnitude)

Ranking metrics -- NDCG, MAP, Reciprocal Rank
	NDCG - penalize based on position, apply log penalty
	MAP - care about positions of all good recoms
	RR - care about position of the best recom
Char metrics -- Inverse Propensity, Diversity / Coverage, Serendipity (surprise factor) / Novelty

Covariance vs correlation
Covariance measures the linear relationship between two variables. Neg infinity to Pos infinity. 
Does not measure the strength of the relationship. 
Correlation does. Some examples of correlation are Pearson coefficient, euclidian distance coefficient.
multi-variate normal distribution = means of m variables & covariance mxm
multi-variate normal distribution = means of m variables & std of m variables & corr mxm

bias variance tradeoff - 
	model shows high bias towards certain TEST data = underfit 
	model shows high variance in TEST data = overfit (to TRAIN data)
	either remove some of the features [or] 
	use regularization i.e. reduce impact of some of the features. L1 and L2 reg. [or]
	give more data to model / add noise to data

accuracy = ratio of correctly classified samples
	if you have multi-class, accuracy will suffer due to class imbalance
precision = tp/tp+fp ; recall = tp/tp+fn ; specificity = tn/tn+fp
specificity = inverse measure of FN ; sensitivity = measure of FP
sensitivity & recall are SAME
we want high precicison & recall
we want high specificity & sensitivity
AUC tells fp rate wrt tp rate
AUC ROC curve (between FP rate and TP rate) can also be (between FP rate and Precision)
F1 = 2*precision*recall / precision + recall --> higher the better


Production Pipeline
===================
Nightly builds
	Spark data load
	PySpark preprocess
	Python model / Pytorch model
	Argo workflow manager (kubeflow)
	GCP serve
Real-time serve
	JSON request (userid, pid)
	Cached embeddings (userid, pid)
	## Redis / memcached data stores
	Big table querying (row, col, data)
	Cached model inference
	Cached candidates from stage 1 model
	Rerank (numpy.dot)
	GCP serve


DESIGN
======
to personalize = use colab filtering / time series / multi-class classification



TRICKY:
===============================================
linear vs logistic - colinearity assumptions
rnn vs lstm - equations
attention - equations
How to add noise to data to avoid overfit?
Given 2 distribution & value, which distr does value belong to?
Using random projections as labels for FFN when you dont know what to predict
multi modal - combining various kinds of features like images, text etc
graph NN
===============================================
