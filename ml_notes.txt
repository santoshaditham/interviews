BASIC PROBABILITY
=================

Pdf - probability density for a continuous random variable P(x-e < X < x+e)
Cdf - integral of Pdf - probability upto a particular value of the random variable P(X<=x)
hypothesis tests - statistical significance, p-value
p-value - hypothesis test that verifies if observation is purely by chance (<0.05 means null hypothesis is invalid)
critical regions & confidence intervals
uniform distribution is in range [-m, m)
normal distribution has mean +/- variance
std error - if we take multiple data samples from population, we can calc std of means
central limit theorem (CLT) - frequency plots of means will be normal distribution, lesser std if more means

UNDERSTAND DATA
===============
Monte carlo simulation (theory)
	Unbiased estimators on some sampling technique (inverse, rejection etc)
	Metropolis random walk - 
		pick random point Xi from raw 
		pick random point from region around Xi -- Xj
		calc prob (Xj / X) <= uniform(0,1) to select Xj as next state
	Markov chain : xk = x0 * pow(P, k) where P is transition probabilities
	Markov chains stabilize after certain k at which point P needs to be recomputed
Bootstrapping (practical)
	dont want to use raw data, then rely on CLT & use bootstrapping for variance tests 
	select a part of raw data at random -- test set / evaluation set
	re-sample again and again WITH replacement from that part
	in case of biased data, re-scale to center
Inverse sampling
	to generate samples form a known pdf, say f(x)
	calculate inverse cdf of the known pdf
	feed from uniform(0,1) to the inverse cdf to generate random values of x
Rejection sampling
	to generate samples of non-normal pdf (say exponential)
	for 2d space, generate 2 values x & y from uniform space
	reject points above pdf, accept points below pdf
Importance sampling
	to generate samples of unknown pdf
	rewrite unknown pdf using known pdf (say normal distr) + some factor
	do WEIGHTED sampling from known pdf
Stratified sampling
	when you have clusters of data - say views, atb, purchases
	random subsample from each cluster to create random sample
	

ANOVA and Tukey (assume normal distribution)
ANOVA is to measure if 3 or more sample means are different - yes/no. 
It compares overall mean with individual means
Individual samples cannot have big diff in their variance + no outliers
If p-value is less than alpha (typically 0.05), then means are different because null hypothesis that both are same is invalid. But by how much? Use Tukey test which does all pairs comparison.


Chi square test = Sigma (y-y')2/y' --> to measure if variance in output is due to input or not

If two independent distributions X and Y & we want to check if point z is from X or Y
assume X , Y are all from indpendent normal distributions - p(X), p(Y) is given
p(zEX|X) = p(X|zEX).p(z) / P(X)
p(zEY|Y) = p(Y|zEX).p(z) / P(Y)
P(z) = p(zEX|X).p(X) + p(zEY|Y).p(Y) 
P(X and Y) = P(X) + P(Y) + P(X or Y)
	

DATA MINING
===========

Apriori - scan db multiple times to find patterns based on counts
	generates candidates & updates them
	good if unique items is small 
FP growth - scan db twice to build tree of paths + counts


Traditional ML
==============

Kmeans (unsupervised) - assign k labels to data at random ;
compute centroid for each cluster ; re-assign data to centroids based on distance ;
compute variance of cluster ; do repeatedly ; plot elbow curve


KNN (supervised) - have data with labels ; get new unlabeled data ; measure distances ;
pick top k labeled data ; give new label based on max of top k


SVM (support vector machines) for classification -
support vectors are actual margins + small buffer to allow misclassication
do cross validation with diff support vectors
move computations to higher dimensions using kernel functions & dot product
	
	
linear regression - used for gueustimating the relationship between two variables x and y as accurately as possible. 
Naturally, there is some error while doing so. Typically, RMSE is used to prevent +ve, -ve errors canceling each other out. 

y = mx+c = weight*x + bias

logistic regression - for classification into multiple output variables where the relation between input and output variables follows a logistic funciton [0,1]. 
It assumes that log odds of y is linear function of x. Counter parts are - naive bayes, svm, neural nets etc.

log(y/1-y) = mx+c
y = ez / 1 + ez


To calc max likelihood = minimize negative log likelihood
Loss = KL divergence, cross-entropy (for classification) [or] RMSE (regression)
Entropy = Negative log probability. 
Cross entropy is avg entropy size. 
Divergence is (cross entropy - entropy).
derivative of linear (y=mx+c) gives convex output -> RMSE works  
but derivative of logistic (negative log sigmoid) isnt convex -> RMSE does not work 

Listwise, Pairwise & Pointwise loss calculation --> maybe even use Triplet loss


Naive Bayes - probabilistic classification model that assumes features are independent.
P(x|y) = P(y|x).P(x) / P(y)


random forest vs xgboost (classification and/or regression)
Decision trees rely on entropy which is average of variance - they calculate info gain & prune leaves.
Rf is bagging technique, learn individual weak learners & avg. 
	Bootstapping is randomly selecting data from raw
	Build bunch of decision trees for randomly picked features from randomly picked data (bootstrap)
	Increase / Decrease number of picked features & do again
	Aggregate the results from all trees to finalize
	Bagging = bootsrap + aggregate to prevent overfit
	Check out-of-bag data accuracy (look at all trees) 
	Missing data for features need to be guessed
XGboost is boosting technique, learn weak learners in sequence + use weights.
	Start with a random leaf & build RESIDUAL/ERROR tree
	Calculate scores, gain & compare gain with some defined gamma to prune (or not)
	Continue building CHAIN of residual trees (more features)
	Each tree is small (say 4) 
	Aggregate values by averaging features (regression) / log odds (classification)
	Calculate predictions using learning rate (avoid overfit)
	Use regularization while calculating scores & outputs
	
Linear, Logistic, Trees --> assume linear decision boundaries	
Trees can use data as is 
SVM, DL --> can do non-linear boundaries
DL needs data to be transformed into meaningful space
	

gradient descent - used to minimize the error in predicted output. 
Depends on math that derivative [or] rate of change of zero means that it is a point of local minima or maxima. 
Uses a param called learning rate which tells how quickly the algo should learn to converge. 
momentum = direction in which GD should keep going, based on previous directions
learning rate = size of the step to take (can be adaptive)


Covariance vs correlation
Covariance measures the linear relationship between two variables. Neg infinity to Pos infinity. 
Does not measure the strength of the relationship. Correlation does. 
Some examples of correlation are Pearson coefficient, euclidian distance coefficient.


bias variance tradeoff - high bias & underfit; high variance & overfit. 
So, either remove some of the features [or] 
use regularization i.e. reduce impact of some of the features. L1 and L2 reg. [or]
give more data to model


Make sure to feature engineer on TRAIN data, not ALL data
Cross validation - k fold, split data in k groups. Train on k-1 groups & test on the kth.
Bootstrap from test data chunk while measuring accuracy of model

precision = tp/tp+fp ; recall = tp/tp+fn
we want high precicison & recall
specificity = inverse measure of FN ; sensitivity = measure of FP
we want high specificity & sensitivity
AUC tells fp rate wrt tp rate
AUC ROC curve (between FP rate and TP rate) can also be (between FP rate and Precision)


Collab filtering - item item, user user or user item.
matrix factorization - dim reduction. Like PCA. dim reduc (non-negative + coordinate descent)
svd, pca - dimensionality reduction. PC is direction in which there is most variance.
Eigen values, Eigen vectors - when linear transforming data to lower dim, eigen value is best multiplier such that
M * eigenvector = eigenvalue * eigenvector

implicit ALS = alternate between user and item vectors, give weights to event types
L2 norm for regularization
confidence calcualtions using imu/umi/filters/popularity

Distance - eucledian, dot product, cosine similarity etc
dot product is angle & magnitude. cosine is angle only (norm of magnitude)

Ranking metrics -- NDCG, MAP, Reciprocal Rank
Char metrics -- Inverse Propensity, Diversity / Coverage, Serendipity (surprise factor) / Novelty


NEURAL NETWORKS
===============

cost function = sum (input * weight) + bias
non-linear activations to excite certain output = sigmoid, softmax, tanh, relu ; sigmoid [0,1] vs tanh[-1,1] vs relu[0,1]
gradient descent = which direction towards 0 & how much step to take 
optimizer (for a cost function) = adam, sgd, gd, batch gd, momentum, adaptive learning rate ; 
	Adam is using momentum. 
	Stochastic GD is compute cost at every step instead of cost for all.
loss = RMSE (regression) or Cross Entropy (classification)
back prop = weighted sum of previous activations + bias (chain rule)
learned params = weights & bias (both can have noise added to them to avoid overfit)


high bias in model = underfit
high variance in data + model learning train data = overfit
regularization (L1 and L2) of weights, dropout, more train data (maybe add noise)


RNN, LSTM, GRU - 
	non-linear models (tanh / relu activations) 
	continuous representation of everything so far (previous output + current input)
	RNN has vanishing gradient problem when sequence is too long
	LSTM, GRU use sigmoid & tanh gates to remember/forget stuff 
	To predict a value, look at seq before the value & after the value -- BiLSTM
	cannot parallelize
Encode-Decoder -
	encoder goes thru time steps & puts everything to context vector
	decoder starts with a context vector and generates next time steps
	adding attention lets you focus on some of the many time steps
Attention - 
	compare with neighborhood to find which are important
	do matmul between embeddings and decider (say position)
	softmax to get areas of focus (multi class)
	summarize (add) & give it to FFNN


TIME SERIES
===========

ARIMA is simple (linear) time series forecasting technique
	based on moving averages & regression
	small data & immediate forecasting
	trends fit some distribution - add [or] multiply raw data with something
	seasonality repeats itself - need time window
	can try more complex EXPONENTIAL SMOOTHENING
	
HMM (Hidden Markov Models) for variable length time-series data -
	linear model to predict sequences using prior probabilities & state machines
	good when next state depends only on previous state (so define prev state carefully)
	use when final states are finite set
	has forward/backward learning but very simple model compared to DL
	
LSTM is the new kid on the block



NLP 
===
stopwords, stemming, lemmatization
POS tags can be created by building state machines on grammar rules
N-grams based bag-of-words model is typically step1 for any model like tfidf / lda
Tfidf computes term freq / inverse doc freq -- can have various flavours
LDA assumes dirchilect prior on topics
word2vec - cbow vs skipgram - both used for context based prediction. 
	cbow prefers high frequency word patterns & it is faster.
	out of vocab cannot be handled - use char level embeddings / some bi-lstm
Bert - uses transformer model for sequence to sequence 



Production Pipeline
===================
Nightly builds
	Spark data load
	PySpark preprocess
	Python model / Pytorch model
	Argo workflow manager (kubeflow)
	GCP serve
Real-time serve
	JSON request (userid, pid)
	Cached embeddings (userid, pid)
	## Redis / memcached data stores
	Big table querying (row, col, data)
	Cached model inference
	Cached candidates from stage 1 model
	Rerank (numpy.dot)
	GCP serve


TRICKY:
===============================================
linear vs logistic - colinearity assumptions
rnn vs lstm - equations
attention - equations
How to add noise to data to avoid overfit?
Given 2 distribution & value, which distr does value belong to?
===============================================
